{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOwAoWKRj7yd",
        "outputId": "82618dae-01c2-401f-a188-fa75dd350a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m849.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.0.0\n",
            "    Uninstalling gymnasium-1.0.0:\n",
            "      Successfully uninstalled gymnasium-1.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1\n",
            "Collecting minatar==1.0.15\n",
            "  Downloading MinAtar-1.0.15-py3-none-any.whl.metadata (685 bytes)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (1.4.8)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (2.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (0.13.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from minatar==1.0.15) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar==1.0.15) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar==1.0.15) (4.55.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar==1.0.15) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar==1.0.15) (11.1.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->minatar==1.0.15) (2025.1)\n",
            "Downloading MinAtar-1.0.15-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: minatar\n",
            "Successfully installed minatar-1.0.15\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.36.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.1.0)\n"
          ]
        }
      ],
      "source": [
        "# @title Install Requirements\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install minatar==1.0.15\n",
        "!pip install matplotlib\n",
        "!pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "xZGQt2UNkIpg"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable\n",
        "from collections import namedtuple, deque\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lXYZM6UfkxL0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, obs_shape: torch.Size, num_actions: int):\n",
        "        \"\"\"\n",
        "        Initialize the DQN network.\n",
        "\n",
        "        :param obs_shape: Shape of the observation space\n",
        "        :param num_actions: Number of actions\n",
        "        \"\"\"\n",
        "\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        if len(obs_shape) == 3:\n",
        "            obs_shape = (1, *obs_shape)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=obs_shape[-1], out_channels=16, kernel_size=5, stride=1, padding='same')\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=32 * obs_shape[1] * obs_shape[2], out_features=128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.output = nn.Linear(in_features=128, out_features=num_actions)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.relu4(self.fc1(x))\n",
        "        x = self.output(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sMIi7sg_k7dS"
      },
      "outputs": [],
      "source": [
        "# @title Policy\n",
        "def make_epsilon_greedy_policy(Q: nn.Module, num_actions: int):\n",
        "    \"\"\"\n",
        "    Creates an epsilon-greedy policy based on a given Q-function and epsilon. Taken from last exercise with changes.\n",
        "\n",
        "    :param Q: The DQN network.\n",
        "    :param num_actions: Number of actions in the environment.\n",
        "\n",
        "    :returns: A function that takes the observation as an argument and returns the greedy action in form of an int.\n",
        "    \"\"\"\n",
        "\n",
        "    def policy_fn(obs: torch.Tensor, epsilon: float = 0.0):\n",
        "        \"\"\"This function takes in the observation and returns an action.\"\"\"\n",
        "        if np.random.uniform() < epsilon:\n",
        "            return np.random.randint(0, num_actions)\n",
        "\n",
        "        return Q(obs).argmax().detach().numpy()\n",
        "\n",
        "    return policy_fn\n",
        "\n",
        "def linear_epsilon_decay(eps_start: float, eps_end: float, current_timestep: int, duration: int) -> float:\n",
        "    \"\"\"\n",
        "    Linear decay of epsilon.\n",
        "\n",
        "    :param eps_start: The initial epsilon value.\n",
        "    :param eps_end: The final epsilon value.\n",
        "    :param current_timestep: The current timestep.\n",
        "    :param duration: The duration of the schedule (in timesteps). So when schedule_duration == current_timestep, eps_end should be reached\n",
        "\n",
        "    :returns: The current epsilon.\n",
        "    \"\"\"\n",
        "\n",
        "    epsilon = eps_start - (eps_start - eps_end) * (current_timestep / duration)\n",
        "    return max(epsilon, eps_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DyXuY7RMlz9R"
      },
      "outputs": [],
      "source": [
        "# @title Replay Buffer\n",
        "from dataclasses import replace\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size: int, n_step: int, gamma: float):\n",
        "        \"\"\"\n",
        "        Create a multi-step replay buffer.\n",
        "\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        :param n_step: Number of steps for multi-step returns.\n",
        "        :param gamma: Discount factor for calculating multi-step returns.\n",
        "        \"\"\"\n",
        "        self.replay_buffer = deque(maxlen=max_size)\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
        "        return len(self.replay_buffer)\n",
        "\n",
        "\n",
        "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Adds a new transition to the n-step buffer and stores the multi-step return in the replay buffer when enough steps are collected.\n",
        "\n",
        "        :param obs: The current observation.\n",
        "        :param action: The action.\n",
        "        :param reward: The reward.\n",
        "        :param next_obs: The next observation.\n",
        "        :param terminated: Whether the episode terminated.\n",
        "        \"\"\"\n",
        "        self.replay_buffer.append((obs, action, reward, next_obs, terminated))\n",
        "\n",
        "\n",
        "    def sample(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
        "\n",
        "        :param batch_size: The batch size.\n",
        "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
        "        \"\"\"\n",
        "\n",
        "        indices = random.choices(range(len(self.replay_buffer)), k=batch_size)\n",
        "\n",
        "        obs_batch = []\n",
        "        action_batch = []\n",
        "        next_obs_batch = []\n",
        "        reward_batch = []\n",
        "        terminated_batch = []\n",
        "        batch = []\n",
        "\n",
        "        for i in indices:\n",
        "            sum_reward = 0\n",
        "            states_look_ahead = self.replay_buffer[i][3]\n",
        "            done_look_ahead = self.replay_buffer[i][4]\n",
        "\n",
        "            # N-step look ahead loop to compute the reward and pick the new 'next_state' (of the n-th state)\n",
        "            for n in range(self.n_step):\n",
        "                if len(self.replay_buffer) > i+n:\n",
        "                    # compute the n-th reward\n",
        "                    sum_reward += (self.gamma**n) * self.replay_buffer[i+n][2]\n",
        "                    if self.replay_buffer[i+n][4]: # next obs is terminated\n",
        "                        states_look_ahead = self.replay_buffer[i+n][3]\n",
        "                        done_look_ahead = torch.tensor(True)\n",
        "                        break\n",
        "                    else:\n",
        "                        states_look_ahead = self.replay_buffer[i+n][3]\n",
        "                        done_look_ahead = torch.tensor(False)\n",
        "\n",
        "            batch.append((self.replay_buffer[i][0], self.replay_buffer[i][1], sum_reward, states_look_ahead, done_look_ahead))\n",
        "\n",
        "        obs_batch = torch.stack([t[0] for t in batch])\n",
        "        action_batch = torch.stack([t[1] for t in batch])\n",
        "        reward_batch = torch.stack([t[2] for t in batch])\n",
        "        next_obs_batch = torch.stack([t[3] for t in batch])\n",
        "        terminated_batch = torch.stack([t[4] for t in batch])\n",
        "\n",
        "        return (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0YNgJum0l3Ds"
      },
      "outputs": [],
      "source": [
        "# @title Update DQN\n",
        "def update_dqn(\n",
        "        q: nn.Module,\n",
        "        q_target: nn.Module,\n",
        "        optimizer: optim.Optimizer,\n",
        "        gamma: float,\n",
        "        obs: torch.Tensor,\n",
        "        act: torch.Tensor,\n",
        "        rew: torch.Tensor,\n",
        "        next_obs: torch.Tensor,\n",
        "        tm: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Update the DQN network for one optimizer step.\n",
        "\n",
        "    :param q: The DQN network.\n",
        "    :param q_target: The target DQN network.\n",
        "    :param optimizer: The optimizer.\n",
        "    :param gamma: The discount factor.\n",
        "    :param obs: Batch of current observations.\n",
        "    :param act: Batch of actions.\n",
        "    :param rew: Batch of rewards.\n",
        "    :param next_obs: Batch of next observations.\n",
        "    :param tm: Batch of termination flags.\n",
        "\n",
        "    \"\"\"\n",
        "    optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        next_q_value = q_target(next_obs).max(dim=1)[0]\n",
        "        td_target = rew + (gamma * next_q_value)\n",
        "\n",
        "    current_q_value = q(obs).gather(1, act.unsqueeze(1))\n",
        "\n",
        "    loss = nn.MSELoss()(current_q_value, td_target.unsqueeze(1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z4K-gD7el7hZ"
      },
      "outputs": [],
      "source": [
        "# @title DQN Agent\n",
        "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self,\n",
        "            env,\n",
        "            gamma=0.9,\n",
        "            lr=0.001,\n",
        "            batch_size=64,\n",
        "            eps_start=1.0,\n",
        "            eps_end=0.1,\n",
        "            schedule_duration=1_000,\n",
        "            update_freq=100,\n",
        "            maxlen=100_000,\n",
        "            n_step=5,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initialize the DQN agent.\n",
        "\n",
        "        :param env: The environment.\n",
        "        :param gamma: The discount factor.\n",
        "        :param lr: The learning rate.\n",
        "        :param batch_size: Mini batch size.\n",
        "        :param eps_start: The initial epsilon value.\n",
        "        :param eps_end: The final epsilon value.\n",
        "        :param schedule_duration: The duration of the schedule (in timesteps).\n",
        "        :param update_freq: How often to update the Q target.\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_start = eps_start\n",
        "        self.eps_end = eps_end\n",
        "        self.schedule_duration = schedule_duration\n",
        "        self.update_freq = update_freq\n",
        "        self.n_step = n_step\n",
        "\n",
        "        # Initialize the Replay Buffer\n",
        "        self.replay_buffer = ReplayBuffer(max_size=maxlen, n_step=self.n_step, gamma=gamma)\n",
        "\n",
        "        # Initialize the Deep Q-Network\n",
        "        self.q = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "        # Initialize the second Q-Network, the target network. Load the parameters of the first one into the second\n",
        "        self.q_target = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "        params = copy.deepcopy(self.q.state_dict())\n",
        "        self.q_target.load_state_dict(params)\n",
        "\n",
        "        # Create an ADAM optimizer for the Q-network\n",
        "        self.optimizer = optim.Adam(self.q.parameters(), lr=lr)\n",
        "\n",
        "        self.policy = make_epsilon_greedy_policy(self.q, env.action_space.n)\n",
        "\n",
        "\n",
        "    def train(self, num_episodes: int) -> EpisodeStats:\n",
        "        \"\"\"\n",
        "        Train the DQN agent.\n",
        "\n",
        "        :param num_episodes: Number of episodes to train.\n",
        "        :returns: The episode statistics.\n",
        "        \"\"\"\n",
        "        # Keeps track of useful statistics\n",
        "        stats = EpisodeStats(\n",
        "            episode_lengths=np.zeros(num_episodes),\n",
        "            episode_rewards=np.zeros(num_episodes)\n",
        "        )\n",
        "        current_timestep = 0\n",
        "        epsilon = self.eps_start\n",
        "\n",
        "        for i_episode in range(num_episodes):\n",
        "            # Print out which episode we're on, useful for debugging.\n",
        "            if (i_episode + 1) % self.update_freq == 0:\n",
        "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}  Epsilon: {epsilon:.3f}')\n",
        "\n",
        "            # Reset the environment and get initial observation\n",
        "            obs, _ = self.env.reset()\n",
        "\n",
        "            for episode_time in itertools.count():\n",
        "\n",
        "                epsilon = linear_epsilon_decay(self.eps_start, self.eps_end, current_timestep, self.schedule_duration)\n",
        "\n",
        "                action = self.policy(torch.as_tensor(obs).unsqueeze(0).float(), epsilon=epsilon)\n",
        "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "                stats.episode_rewards[i_episode] += reward\n",
        "                stats.episode_lengths[i_episode] += 1\n",
        "\n",
        "                self.replay_buffer.store(torch.tensor(obs), torch.tensor(action), torch.tensor(reward), torch.tensor(next_obs), torch.tensor(terminated))\n",
        "\n",
        "                # if self.replay_buffer.__len__() >= 1:\n",
        "                obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "                update_dqn(\n",
        "                            self.q,\n",
        "                            self.q_target,\n",
        "                            self.optimizer,\n",
        "                            self.gamma,\n",
        "                            obs_batch.float(),\n",
        "                            act_batch,\n",
        "                            rew_batch.float(),\n",
        "                            next_obs_batch.float(),\n",
        "                            tm_batch)\n",
        "\n",
        "                # Update the current Q target\n",
        "                if current_timestep % self.update_freq == 0:\n",
        "                    self.q_target.load_state_dict(self.q.state_dict())\n",
        "\n",
        "                current_timestep += 1\n",
        "\n",
        "                # Check whether the episode is finished\n",
        "                if terminated or truncated or episode_time >= 500:\n",
        "                    break\n",
        "                obs = next_obs\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbjcsZdul-Jn"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "\n",
        "# Choose your environment\n",
        "env = gym.make('MinAtar/Breakout-v1', render_mode=\"rgb_array\")\n",
        "\n",
        "# Print observation and action space infos\n",
        "print(f\"Training on {env.spec.id}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_BUFFER_SIZE = 10_000\n",
        "UPDATE_FREQ = 100\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "SCHEDULE_DURATION = 13_000\n",
        "NUM_EPISODES = 1_000\n",
        "DISCOUNT_FACTOR = 0.8\n",
        "N_Step = 3\n",
        "\n",
        "# Train DQN\n",
        "agent = DQNAgent(\n",
        "                env,\n",
        "                gamma=DISCOUNT_FACTOR,\n",
        "                lr=LR,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                eps_start=EPS_START,\n",
        "                eps_end=EPS_END,\n",
        "                schedule_duration=SCHEDULE_DURATION,\n",
        "                update_freq=UPDATE_FREQ,\n",
        "                maxlen=REPLAY_BUFFER_SIZE,\n",
        "                n_step=N_Step\n",
        ")\n",
        "stats = agent.train(NUM_EPISODES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW2YEE77wUON"
      },
      "outputs": [],
      "source": [
        "# @title Plots\n",
        "\n",
        "smoothing_window=20\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
        "\n",
        "# Plot the episode length over time\n",
        "ax = axes[0]\n",
        "ax.plot(stats.episode_lengths)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Length\")\n",
        "ax.set_title(\"Episode Length over Time\")\n",
        "\n",
        "# Plot the episode reward over time\n",
        "ax = axes[1]\n",
        "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "ax.plot(rewards_smoothed)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
        "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PPNjL8CmDEP"
      },
      "outputs": [],
      "source": [
        "# @title The Final Policy\n",
        "\n",
        "from IPython.display import Image as IImage\n",
        "\n",
        "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
        "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
        "    # Create a list to hold each frame\n",
        "    frames = []\n",
        "\n",
        "    # Convert RGB arrays to PIL Image objects\n",
        "    for rgb_array in rgb_arrays:\n",
        "        rgb_array = (rgb_array*255).astype(np.uint8)\n",
        "        rgb_array = rgb_array.repeat(48, axis=0).repeat(48, axis=1)\n",
        "        img = Image.fromarray(rgb_array)\n",
        "        frames.append(img)\n",
        "\n",
        "    # Save the frames as an animated GIF\n",
        "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
        "\n",
        "def rendered_rollout(policy, env, max_steps=1000_000_000):\n",
        "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    imgs = [env.render()]\n",
        "\n",
        "    for i in range(max_steps):\n",
        "        action = policy(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0))\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "        imgs.append(env.render())\n",
        "\n",
        "        if terminated or truncated:\n",
        "            print(i)\n",
        "            break\n",
        "\n",
        "    return imgs\n",
        "\n",
        "policy = make_epsilon_greedy_policy(agent.q, num_actions=env.action_space.n)\n",
        "imgs = rendered_rollout(policy, env)\n",
        "save_rgb_animation(imgs, \"trained.gif\")\n",
        "IImage(filename=\"trained.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Train\n",
        "\n",
        "# # Choose your environment\n",
        "# env = gym.make('MinAtar/Breakout-v1', render_mode=\"rgb_array\")\n",
        "\n",
        "# # Print observation and action space infos\n",
        "# print(f\"Training on {env.spec.id}\")\n",
        "# print(f\"Observation space: {env.observation_space}\")\n",
        "# print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "# LR = 0.001\n",
        "# BATCH_SIZE = 32\n",
        "# REPLAY_BUFFER_SIZE = 10_000\n",
        "# UPDATE_FREQ = 100\n",
        "# EPS_START = 0.9\n",
        "# EPS_END = 0.01\n",
        "# SCHEDULE_DURATION = 13_000\n",
        "# NUM_EPISODES = 1_000\n",
        "# DISCOUNT_FACTOR = 0.8\n",
        "# N_Step = 1\n",
        "\n",
        "# # Train DQN\n",
        "# agent = DQNAgent(\n",
        "#                 env,\n",
        "#                 gamma=DISCOUNT_FACTOR,\n",
        "#                 lr=LR,\n",
        "#                 batch_size=BATCH_SIZE,\n",
        "#                 eps_start=EPS_START,\n",
        "#                 eps_end=EPS_END,\n",
        "#                 schedule_duration=SCHEDULE_DURATION,\n",
        "#                 update_freq=UPDATE_FREQ,\n",
        "#                 maxlen=REPLAY_BUFFER_SIZE,\n",
        "#                 n_step=N_Step\n",
        "# )\n",
        "# stats_1 = agent.train(NUM_EPISODES)\n",
        "\n",
        "# # Print observation and action space infos\n",
        "# print(f\"Training on {env.spec.id}\")\n",
        "# print(f\"Observation space: {env.observation_space}\")\n",
        "# print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "\n",
        "# N_Step = 3\n",
        "\n",
        "# # Train DQN\n",
        "# agent = DQNAgent(\n",
        "#                 env,\n",
        "#                 gamma=DISCOUNT_FACTOR,\n",
        "#                 lr=LR,\n",
        "#                 batch_size=BATCH_SIZE,\n",
        "#                 eps_start=EPS_START,\n",
        "#                 eps_end=EPS_END,\n",
        "#                 schedule_duration=SCHEDULE_DURATION,\n",
        "#                 update_freq=UPDATE_FREQ,\n",
        "#                 maxlen=REPLAY_BUFFER_SIZE,\n",
        "#                 n_step=N_Step\n",
        "# )\n",
        "# stats_3 = agent.train(NUM_EPISODES)\n",
        "\n",
        "\n",
        "\n",
        "# # Print observation and action space infos\n",
        "# print(f\"Training on {env.spec.id}\")\n",
        "# print(f\"Observation space: {env.observation_space}\")\n",
        "# print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "# N_Step = 5\n",
        "\n",
        "# # Train DQN\n",
        "# agent = DQNAgent(\n",
        "#                 env,\n",
        "#                 gamma=DISCOUNT_FACTOR,\n",
        "#                 lr=LR,\n",
        "#                 batch_size=BATCH_SIZE,\n",
        "#                 eps_start=EPS_START,\n",
        "#                 eps_end=EPS_END,\n",
        "#                 schedule_duration=SCHEDULE_DURATION,\n",
        "#                 update_freq=UPDATE_FREQ,\n",
        "#                 maxlen=REPLAY_BUFFER_SIZE,\n",
        "#                 n_step=N_Step\n",
        "# )\n",
        "# stats_5 = agent.train(NUM_EPISODES)\n",
        "\n",
        "\n",
        "# # Print observation and action space infos\n",
        "# print(f\"Training on {env.spec.id}\")\n",
        "# print(f\"Observation space: {env.observation_space}\")\n",
        "# print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "# N_Step = 7\n",
        "\n",
        "# # Train DQN\n",
        "# agent = DQNAgent(\n",
        "#                 env,\n",
        "#                 gamma=DISCOUNT_FACTOR,\n",
        "#                 lr=LR,\n",
        "#                 batch_size=BATCH_SIZE,\n",
        "#                 eps_start=EPS_START,\n",
        "#                 eps_end=EPS_END,\n",
        "#                 schedule_duration=SCHEDULE_DURATION,\n",
        "#                 update_freq=UPDATE_FREQ,\n",
        "#                 maxlen=REPLAY_BUFFER_SIZE,\n",
        "#                 n_step=N_Step\n",
        "# )\n",
        "# stats_7 = agent.train(NUM_EPISODES)\n",
        "\n",
        "\n",
        "# # Print observation and action space infos\n",
        "# print(f\"Training on {env.spec.id}\")\n",
        "# print(f\"Observation space: {env.observation_space}\")\n",
        "# print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "# N_Step = 10\n",
        "\n",
        "# # Train DQN\n",
        "# agent = DQNAgent(\n",
        "#                 env,\n",
        "#                 gamma=DISCOUNT_FACTOR,\n",
        "#                 lr=LR,\n",
        "#                 batch_size=BATCH_SIZE,\n",
        "#                 eps_start=EPS_START,\n",
        "#                 eps_end=EPS_END,\n",
        "#                 schedule_duration=SCHEDULE_DURATION,\n",
        "#                 update_freq=UPDATE_FREQ,\n",
        "#                 maxlen=REPLAY_BUFFER_SIZE,\n",
        "#                 n_step=N_Step\n",
        "# )\n",
        "# stats_10 = agent.train(NUM_EPISODES)\n",
        "\n",
        "\n",
        "\n",
        "# ##################################################################\n",
        "# smoothing_window=40\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
        "\n",
        "# # Plot the episode length over time\n",
        "# ax = axes[0]\n",
        "# ax.plot(stats_3.episode_lengths)\n",
        "# ax.set_xlabel(\"Episode\")\n",
        "# ax.set_ylabel(\"Episode Length\")\n",
        "# ax.set_title(\"Episode Length over Time\")\n",
        "# ax = axes[1]\n",
        "# #################################################################\n",
        "\n",
        "\n",
        "# # smoothing_window=20\n",
        "# # fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)\n",
        "\n",
        "# # Define stats and labels for different n-step values\n",
        "# # stats_list = [stats_1, stats_3, stats_5, stats_7, stats_10]\n",
        "\n",
        "# stats_list = [stats_1, stats_3]\n",
        "# n_step_values = [1, 3]\n",
        "\n",
        "# # n_step_values = [1, 3, 5, 7, 10]\n",
        "\n",
        "# # Plot smoothed episode rewards for each n-step value\n",
        "# for stats, n_step in zip(stats_list, n_step_values):\n",
        "#     rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "#     if n_step == 1:\n",
        "#         ax.plot(rewards_smoothed, label=f\"Normal DQN\")\n",
        "#     else:\n",
        "#         ax.plot(rewards_smoothed, label=f\"{n_step}-Step DQN\")\n",
        "\n",
        "# # Formatting the plot\n",
        "# ax.set_xlabel(\"Episode\")\n",
        "# ax.set_ylabel(\"Smoothed Episode Reward\")\n",
        "# ax.set_title(f\"Comparison of Episode Rewards for Different N-Step Values\\n(Smoothed over {smoothing_window} episodes)\")\n",
        "# ax.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "tjrvmf0rkpyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}